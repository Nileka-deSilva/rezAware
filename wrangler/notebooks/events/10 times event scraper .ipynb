{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbf2179c",
   "metadata": {},
   "source": [
    "# 10 times.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda66be6",
   "metadata": {},
   "source": [
    "### Step 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce3ba7e",
   "metadata": {},
   "source": [
    "Install the ChromeDriver using the link below: \n",
    "\n",
    "https://chromedriver.chromium.org/downloads \n",
    "\n",
    "Download the latest version of Chrome (in case thereâ€™s an error and the event script is not opening the chrome driver, then repeat step 1 and download the older version of it) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901549c7",
   "metadata": {},
   "source": [
    "### Step 2\n",
    "\n",
    "Load necessary and sufficient python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374a068f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup # Import for Beautiful Soup\n",
    "import requests # Import for requests\n",
    "import lxml # Import for lxml parser\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "import csv\n",
    "from csv import writer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import boto3\n",
    "from io import StringIO\n",
    "import time\n",
    "from time import sleep\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException,StaleElementReferenceException\n",
    "from selenium.webdriver.support.ui import WebDriverWait as wait"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83eb638",
   "metadata": {},
   "source": [
    "### Step 3\n",
    "\n",
    "Define the destinations to scrape event data from"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c39d95",
   "metadata": {},
   "source": [
    "Select locations from this list and add it to the code below and run the code line:\n",
    "\n",
    "* \"boston-us\" \n",
    "* \"chicago-us\"\n",
    "* \"lasvegas-us\"\n",
    "* \"newyork-us\"\n",
    "* \"niagarafalls-ca\"\n",
    "* \"orlando-us\"\n",
    "* \"washington-us\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e417e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "destinationIDs= [\"niagarafalls-ca\", \"boston-us\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7021d69a",
   "metadata": {},
   "source": [
    "### Step 4\n",
    "\n",
    "Execute below script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0cdbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Scrape all event title and event URLs and add to df'''\n",
    "\n",
    "for destinationID in destinationIDs:\n",
    "    global driver\n",
    "    try:\n",
    "        main_link = f\"https://10times.com/{destinationID}\"\n",
    "        driver= webdriver.Chrome(r'C:/home/nileka/anaconda3/lib/python3.9/site-packages/selenium/webdriver/chrome/webdriver.py')\n",
    "        driver.get(main_link)\n",
    "        repeated = False\n",
    "        while True:\n",
    "            # Scroll to the bottom of the page\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(2)\n",
    "            # Check if the page has reached the end (no more scrolling possible)\n",
    "            end_of_page = driver.execute_script(\"return window.innerHeight + window.pageYOffset >= document.body.offsetHeight;\")\n",
    "  \n",
    "            driver.execute_script(\"window.scrollTo(0, 0);\") #scroll up \n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")#scroll down again\n",
    "            time.sleep(2)\n",
    "            # Check if the page has reached the end (no more scrolling possible)\n",
    "            end_of_page = driver.execute_script(\"return window.innerHeight + window.pageYOffset >= document.body.offsetHeight;\")\n",
    "            repeated = True\n",
    "            try:\n",
    "                driver.implicitly_wait(0)\n",
    "                button_element= driver.find_element(\"xpath\", '/html/body/div[7]/div/div/div/div[1]/button')\n",
    "                driver.execute_script(\"arguments[0].click();\",button_element)\n",
    "            except NoSuchElementException:\n",
    "                pass            \n",
    "            try:\n",
    "                driver.implicitly_wait(0)\n",
    "                button_element= driver.find_element(\"xpath\", '/html/body/div[6]/div/div/div/div[1]/button')\n",
    "                driver.execute_script(\"arguments[0].click();\",button_element)\n",
    "            except NoSuchElementException:\n",
    "                pass  \n",
    "            \n",
    "            try:\n",
    "                driver.implicitly_wait(0)\n",
    "                button_element= driver.find_element(\"xpath\", '/html/body/div/div[1]/div/div[2]/span')\n",
    "                driver.execute_script(\"arguments[0].click();\",button_element)\n",
    "            except NoSuchElementException:\n",
    "                pass\n",
    "\n",
    "            try:\n",
    "                driver.implicitly_wait(0)\n",
    "                button_element2= driver.find_element(\"xpath\", '/html/body/div[2]/button')\n",
    "                driver.execute_script(\"arguments[0].click();\",button_element2)\n",
    "            except NoSuchElementException:\n",
    "                pass         \n",
    "            \n",
    "            if end_of_page:\n",
    "                break                \n",
    "        \n",
    "        # Make it a soup\n",
    "        soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "        event_titles_df = pd.DataFrame(columns=['Event_Title','Event_URL', 'Event_Type'])    \n",
    "        events= soup.find_all('a', class_='text-decoration-none c-ga xn')\n",
    "        for event in events:\n",
    "            try:\n",
    "                url= event.get('href')\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                name= event.get('data-ga-label')\n",
    "            except:\n",
    "                pass             \n",
    "            try:\n",
    "                event_type= event.get('data-ga-action')\n",
    "            except:\n",
    "                pass                \n",
    "         \n",
    "\n",
    "            event_titles_df = event_titles_df.append({'Event_Title':name,'Event_URL': url, 'Event_Type': event_type}, ignore_index=True)\n",
    "        event_titles_df = event_titles_df.dropna()\n",
    "        event_titles_df= event_titles_df[event_titles_df[\"Event_Title\"].str.contains(\"Event_Title\")==False]\n",
    "        event_titles_df[\"Event_Title\"] = event_titles_df[\"Event_Title\"].apply(lambda x: x.replace(\"To\", \"\"))\n",
    "        event_titles_df= event_titles_df.reset_index(drop=True)\n",
    "        event_titles_df= event_titles_df[event_titles_df['Event_Type'] =='Event Listing | Event Snippet']\n",
    "        event_titles_df=event_titles_df.drop('Event_Type', axis=1)\n",
    "        event_titles_df['Event_URL'] = event_titles_df['Event_URL'].str.replace(' ', '')\n",
    "\n",
    "        links = event_titles_df[\"Event_URL\"]\n",
    "\n",
    "        event_details_df = pd.DataFrame(columns=['Search_Date', 'Date', 'Labels', 'Turnout', 'Latitude', 'Longitude', 'Address', 'Event_URL'])\n",
    "  \n",
    "        for link in links:\n",
    "            try:\n",
    "                driver= webdriver.Chrome(r'C:/home/nileka/anaconda3/lib/python3.9/site-packages/selenium/webdriver/chrome/webdriver.py')\n",
    "                driver.get(link)\n",
    "                def click_button():\n",
    "                    try:\n",
    "                        driver.implicitly_wait(5)\n",
    "                        button_element= driver.find_element(\"xpath\", '/html/body/div/div[1]/div/div[2]/span')\n",
    "                        driver.execute_script(\"arguments[0].click();\",button_element)\n",
    "                    except NoSuchElementException:\n",
    "                        pass\n",
    "\n",
    "                    try:\n",
    "                        driver.implicitly_wait(5)\n",
    "                        button_element2= driver.find_element(\"xpath\", '/html/body/div[2]/button')\n",
    "                        driver.execute_script(\"arguments[0].click();\",button_element2)\n",
    "                    except NoSuchElementException:\n",
    "                        pass\n",
    "\n",
    "\n",
    "\n",
    "                soup2 = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "                information=soup2.find('table', class_='table noBorder mng w-100 trBorder')\n",
    "                location=soup2.find('div', class_='row fs-14 box p-0')\n",
    "                currentDate = datetime.now().strftime(\"%m-%d-%Y\")\n",
    "\n",
    "\n",
    "                event_link = link\n",
    "\n",
    "                repeated = False\n",
    "                while True:  \n",
    "                    try:\n",
    "                        turnout=information.find('a', class_='text-decoration-none').text\n",
    "                    except NoSuchElementException:\n",
    "                        pass\n",
    "                    try:\n",
    "                        labels=information.find(id='hvrout2').text\n",
    "                    except NoSuchElementException:\n",
    "                        pass\n",
    "                    try:\n",
    "                        latitude=location.find('span', id='event_latitude').text\n",
    "                    except NoSuchElementException:\n",
    "                        pass           \n",
    "                    try:\n",
    "                        longitude=location.find('span', id='event_longude').text\n",
    "                    except NoSuchElementException:\n",
    "                        pass\n",
    "                    try:\n",
    "                        venue=location.find('div', class_='mb-1').text\n",
    "                    except NoSuchElementException:\n",
    "                        pass\n",
    "\n",
    "                    try:\n",
    "                        eventDate=soup2.find('span', class_='ms-1').text\n",
    "                    except NoSuchElementException:\n",
    "                        pass\n",
    "\n",
    "                    if eventDate.endswith(\"Followers\"):\n",
    "                        eventDate=soup2.find('div', class_='header_date position-relative text-orange me-5').text\n",
    "                    else:\n",
    "                        break\n",
    "\n",
    "\n",
    "                    if eventDate.strip() == \"\" or turnout.strip() == \"\" or labels.strip() == \"\" or latitude.strip() == \"\" or longitude.strip() == \"\" or venue.strip() == \"\":\n",
    "                        click_button()\n",
    "                        repeated= True\n",
    "                    else:\n",
    "                        break\n",
    "\n",
    "\n",
    "\n",
    "                event_details_df=event_details_df.append({'Search_Date': currentDate, 'Date': eventDate, 'Labels': labels, 'Turnout': turnout, 'Latitude': latitude, 'Longitude': longitude, 'Address': venue, 'Event_URL': event_link}, ignore_index=True)     \n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        df2=event_details_df\n",
    "        df2=df2.drop_duplicates()\n",
    "        df2=df2.replace('N/A',np.NaN)\n",
    "        df2['Address']=df2['Address'].str.slice(start=2)\n",
    "        df2['Address'] = df2['Address'].str.replace(\"nue\", \"Venue\")\n",
    "        df2['Date'] = df2['Date'].str.replace('LIVE', '')\n",
    "        df2 = df2.reset_index(drop=True)\n",
    "        df2['Date']=df2['Date'].str.replace(\"Add a Review\",\"\")\n",
    "        df2['Date']=df2['Date'].str.replace(\"Add a review\",\"\")\n",
    "\n",
    "        #clean date column and define start date and end date \n",
    "        dateRange= pd.DataFrame(df2['Date'].str.split('-',1).to_list(),columns = ['Start date','End date'])\n",
    "        dateRange['End date'].fillna(dateRange['Start date'], inplace=True)\n",
    "        dateRange[['End day','End month','End year']] = dateRange['End date'].str.extract(r\"^(.*)\\s+(\\S+)\\s+(\\d+)$\", expand=True)\n",
    "        dateRange[['Start day','Start month','Start year']] = dateRange['Start date'].str.extract(r\"^(.*)\\s+(\\S+)*\\s+(\\d+)*$\", expand=True)\n",
    "        dateRange['Start day'].fillna(dateRange['Start date'], inplace=True)\n",
    "        dateRange['Start month'].fillna(dateRange['End month'], inplace=True)\n",
    "        dateRange['Start year'].fillna(dateRange['End year'], inplace=True)\n",
    "        dateRange['Start']=dateRange['Start day'].astype(str)+ \" \"+ dateRange['Start month'].astype(str)+\" \"+ dateRange['Start year'].astype(str)\n",
    "        dateRange['End']=dateRange['End day'].astype(str)+\" \"+dateRange['End month'].astype(str)+\" \"+dateRange['End year'].astype(str)\n",
    "        df2['Start date']= pd.to_datetime(dateRange['Start'])\n",
    "        df2['End date']= pd.to_datetime(dateRange['End'])\n",
    "        del df2['Date']  \n",
    "\n",
    "\n",
    "        df2['Labels']=df2['Labels'].str.replace(\"Category & Type\",\"\")\n",
    "\n",
    "        df2['Category'] = df2['Labels'].str.extract('(Conference|Trade Show)', expand=False)\n",
    "\n",
    "        df2['Labels']=df2['Labels'].str.replace(\"IT\",\"Information Technology\")\n",
    "        df2['Labels']=df2['Labels'].str.replace('Conference', '')\n",
    "        df2['Labels']=df2['Labels'].str.replace('Trade Show', '')\n",
    "        df2['Labels'] = df2['Labels'].replace(r\"(\\w)([A-Z])\", r\"\\1 | \\2\", regex=True)\n",
    "        df2= df2.assign(Labels=df2['Labels'].str.split('|')).explode('Labels')\n",
    "        df2.reset_index(inplace=True)\n",
    "        df2.index = np.arange(1, len(df2) + 1)\n",
    "        df2['Turnout']=df2['Turnout'].str.replace(\"&\",\"\").replace(\"([A-Z][a-z]+)\", \"\").replace(\"IT\", \"\")\n",
    "        df2['Turnout']=df2['Turnout'].str.replace(\"([A-Z][a-z]+)\", \"\", regex=True)\n",
    "        df2['Turnout']=df2['Turnout'].str.replace(\"IT\", \"\")\n",
    "        df2['Turnout']=df2['Turnout'].replace(r'^\\s*$', np.nan, regex=True)\n",
    "        df2 = df2.rename(columns = {'index':'Event ID'})\n",
    "\n",
    "        destination_events_df = pd.merge(event_titles_df, df2, on='Event_URL')\n",
    "        currentDate = datetime.now().strftime(\"%m-%d-%Y\")\n",
    "        destination_events_df.to_csv(f\"Events_in_{destinationID}_10TimesSite_{currentDate}.csv\", index = False)\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "        \n",
    "driver.quit()      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c0602e",
   "metadata": {},
   "source": [
    "### Step 5\n",
    "\n",
    "Once the script finishes running (once the code in the above cell as been executed successfully), check your folder for below file that will be automatically downloaded as a csv file: \n",
    "\n",
    "â€œEvents_in_{destinationID}_10TimesSite_{currentDate}.csv\" "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
