{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72dbb7dd",
   "metadata": {},
   "source": [
    "# Document the Data Sources\n",
    "* [Coingecko python code](https://github.com/man-c/pycoingecko)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20d8f07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    WARNING CONTROL to display or ignore all warnings\n",
    "'''\n",
    "import warnings; warnings.simplefilter('ignore')     #switch betweeb 'default' and 'ignore'\n",
    "import traceback\n",
    "\n",
    "''' Set debug flag to view extended error messages; else set it to False to turn off debugging mode '''\n",
    "debug = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13e7aeb",
   "metadata": {},
   "source": [
    "## Initialize classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c0b673c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All functional APP-libraries in REZAWARE-package of REZAWARE-module imported successfully!\n",
      "All assets-module etl-packages in function-CryptoMarket imported successfully!\n",
      "All functional SPARKNOSQLWLS-libraries in LOADER-package of ETL-module imported successfully!\n",
      "All functional SPARKDBWLS-libraries in LOADER-package of ETL-module imported successfully!\n",
      "sparkNoSQLwls Class initialization complete\n",
      "sparkNoSQLwls Class initialization complete\n",
      "FileWorkLoads Class initialization complete\n",
      "CryptoMarket Class initialization complete\n",
      "\n",
      "Class initialization and load complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import configparser\n",
    "from datetime import datetime, date, timedelta\n",
    "\n",
    "sys.path.insert(1,\"/home/nuwan/workspace/rezaware/\")\n",
    "import rezaware as reza\n",
    "from wrangler.modules.assets.etl import cryptoMCExtractor as mcap\n",
    "from utils.modules.etl.loader import sparkDBwls as spark\n",
    "from utils.modules.etl.loader import sparkNoSQLwls as nosql\n",
    "\n",
    "''' restart initiate classes '''\n",
    "if debug:\n",
    "    import importlib\n",
    "    reza = importlib.reload(reza)\n",
    "    mcap = importlib.reload(mcap)\n",
    "    nosql = importlib.reload(nosql)\n",
    "    spark = importlib.reload(spark)\n",
    "    \n",
    "__desc__ = \"get crypto macket capitalization data\"\n",
    "clsNoSQL = nosql.NoSQLWorkLoads(desc=__desc__)\n",
    "# clsSpark = spark.SQLWorkLoads(desc=__desc__)\n",
    "''' optional - if not specified class will use the default values '''\n",
    "# prop_kwargs = {\"WRITE_TO_TMP\":True,   # necessary to emulate the etl dag\n",
    "#               }\n",
    "clsMC = mcap.CryptoMarkets(desc=__desc__)\n",
    "print(\"\\nClass initialization and load complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23055593",
   "metadata": {},
   "source": [
    "## Transform collections into Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8dddb199",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieving collections ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9243:>                                                       (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/26 14:07:20 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 128299 ms exceeds timeout 120000 ms\n",
      "23/04/26 14:07:20 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.rpc.RpcTimeoutException: Futures timed out after [10000 milliseconds]. This timeout is controlled by spark.executor.heartbeatInterval\n",
      "\tat org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:76)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1053)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:238)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.util.concurrent.TimeoutException: Futures timed out after [10000 milliseconds]\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:259)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:263)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:293)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\t... 12 more\n",
      "23/04/26 14:07:21 WARN NettyRpcEnv: Ignored message: true\n",
      "23/04/26 14:07:21 WARN NettyRpcEnv: Ignored message: HeartbeatResponse(true)\n",
      "23/04/26 14:07:21 WARN NettyRpcEnv: Ignored message: true\n",
      "23/04/26 14:07:21 WARN NettyRpcEnv: Ignored message: true\n",
      "23/04/26 14:07:21 WARN NettyRpcEnv: Ignored message: true\n",
      "23/04/26 14:07:21 WARN SparkContext: Killing executors is not supported by current scheduler.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/26 14:37:13 ERROR Executor: Exception in task 0.0 in stage 29192.0 (TID 20069)\n",
      "java.sql.BatchUpdateException: Batch entry 0 INSERT INTO warehouse.mcap_past (\"uuid\",\"data_source\",\"alt_asset_id\",\"asset_symbol\",\"asset_name\",\"mcap_value\",\"mcap_date\",\"currency\",\"price_value\",\"price_date\",\"volume_size\",\"volume_date\",\"created_proc\",\"created_dt\",\"created_by\") VALUES ('64476a6214c8ee91f463884a','coingecko','phoenix-protocol-b7a9513c-36e9-4a6b-b6ae-6a1a76bb913e','pp','phoenix_protocol',548489.0275745493,'2023-03-02 00:00:00+08'::timestamp,'usd',0.013289691884763082,'2023-03-02 00:00:00+08'::timestamp,5665.271138273307,'2023-03-02 00:00:00+08'::timestamp,'wrangler_assets_etl_CryptoMarket function <nosql_to_sql>','2023-04-26 14:37:13.072953+08'::timestamp,'farmraider') was aborted: ERROR: value too long for type character varying(50)  Call getNextException to see other errors in the batch.\n",
      "\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)\n",
      "\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)\n",
      "\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)\n",
      "\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:713)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:868)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:867)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.postgresql.util.PSQLException: ERROR: value too long for type character varying(50)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)\n",
      "\t... 18 more\n",
      "23/04/26 14:37:13 WARN TaskSetManager: Lost task 0.0 in stage 29192.0 (TID 20069) (192.168.124.15 executor driver): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO warehouse.mcap_past (\"uuid\",\"data_source\",\"alt_asset_id\",\"asset_symbol\",\"asset_name\",\"mcap_value\",\"mcap_date\",\"currency\",\"price_value\",\"price_date\",\"volume_size\",\"volume_date\",\"created_proc\",\"created_dt\",\"created_by\") VALUES ('64476a6214c8ee91f463884a','coingecko','phoenix-protocol-b7a9513c-36e9-4a6b-b6ae-6a1a76bb913e','pp','phoenix_protocol',548489.0275745493,'2023-03-02 00:00:00+08'::timestamp,'usd',0.013289691884763082,'2023-03-02 00:00:00+08'::timestamp,5665.271138273307,'2023-03-02 00:00:00+08'::timestamp,'wrangler_assets_etl_CryptoMarket function <nosql_to_sql>','2023-04-26 14:37:13.072953+08'::timestamp,'farmraider') was aborted: ERROR: value too long for type character varying(50)  Call getNextException to see other errors in the batch.\n",
      "\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)\n",
      "\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)\n",
      "\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)\n",
      "\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:713)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:868)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:867)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.postgresql.util.PSQLException: ERROR: value too long for type character varying(50)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)\n",
      "\t... 18 more\n",
      "\n",
      "23/04/26 14:37:13 ERROR TaskSetManager: Task 0 in stage 29192.0 failed 1 times; aborting job\n",
      "[Error]sparkDBwls function <insert_sdf_into_table> An error occurred while calling o96757.save.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 29192.0 failed 1 times, most recent failure: Lost task 0.0 in stage 29192.0 (TID 20069) (192.168.124.15 executor driver): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO warehouse.mcap_past (\"uuid\",\"data_source\",\"alt_asset_id\",\"asset_symbol\",\"asset_name\",\"mcap_value\",\"mcap_date\",\"currency\",\"price_value\",\"price_date\",\"volume_size\",\"volume_date\",\"created_proc\",\"created_dt\",\"created_by\") VALUES ('64476a6214c8ee91f463884a','coingecko','phoenix-protocol-b7a9513c-36e9-4a6b-b6ae-6a1a76bb913e','pp','phoenix_protocol',548489.0275745493,'2023-03-02 00:00:00+08'::timestamp,'usd',0.013289691884763082,'2023-03-02 00:00:00+08'::timestamp,5665.271138273307,'2023-03-02 00:00:00+08'::timestamp,'wrangler_assets_etl_CryptoMarket function <nosql_to_sql>','2023-04-26 14:37:13.072953+08'::timestamp,'farmraider') was aborted: ERROR: value too long for type character varying(50)  Call getNextException to see other errors in the batch.\n",
      "\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)\n",
      "\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)\n",
      "\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)\n",
      "\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:713)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:868)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:867)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.postgresql.util.PSQLException: ERROR: value too long for type character varying(50)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)\n",
      "\t... 18 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1011)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n",
      "\tat org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1009)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:867)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:70)\n",
      "\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n",
      "\tat jdk.internal.reflect.GeneratedMethodAccessor94.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.sql.BatchUpdateException: Batch entry 0 INSERT INTO warehouse.mcap_past (\"uuid\",\"data_source\",\"alt_asset_id\",\"asset_symbol\",\"asset_name\",\"mcap_value\",\"mcap_date\",\"currency\",\"price_value\",\"price_date\",\"volume_size\",\"volume_date\",\"created_proc\",\"created_dt\",\"created_by\") VALUES ('64476a6214c8ee91f463884a','coingecko','phoenix-protocol-b7a9513c-36e9-4a6b-b6ae-6a1a76bb913e','pp','phoenix_protocol',548489.0275745493,'2023-03-02 00:00:00+08'::timestamp,'usd',0.013289691884763082,'2023-03-02 00:00:00+08'::timestamp,5665.271138273307,'2023-03-02 00:00:00+08'::timestamp,'wrangler_assets_etl_CryptoMarket function <nosql_to_sql>','2023-04-26 14:37:13.072953+08'::timestamp,'farmraider') was aborted: ERROR: value too long for type character varying(50)  Call getNextException to see other errors in the batch.\n",
      "\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)\n",
      "\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)\n",
      "\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)\n",
      "\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:713)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:868)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:867)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\t... 1 more\n",
      "Caused by: org.postgresql.util.PSQLException: ERROR: value too long for type character varying(50)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)\n",
      "\t... 18 more\n",
      "\n",
      "Done processing with 51817 rows\n"
     ]
    }
   ],
   "source": [
    "# _data_owner='coingecko'\n",
    "__SOURCEDBNAME__ = 'tip-historic-marketcap'\n",
    "__DESTINDBNAME__ = 'tip'\n",
    "__DESTINTABLE__ = 'mcap_past'\n",
    "\n",
    "_from_dt=date(2023,3,1)\n",
    "_to_dt=date(2023,3,31)\n",
    "_kwargs = {\n",
    "#     \"SOURCEDBNAME\":'tip-historic-marketcap',\n",
    "    \"DESTINTBLNAME\":'mcap_past',\n",
    "    \"DBAUTHSOURCE\":'tip-historic-marketcap',\n",
    "#     \"COLLLIST\":['coingecko.2022-04-01.btc','coingecko.2022-04-01.etc'],\n",
    "#     \"COLLLIST\":['coingecko.2022-07-01.axial'],\n",
    "   \"HASINNAME\":['coingecko','2023-03-01'],\n",
    "#     \"FIND\":{'symbol':{'$eq':'btc'}},   # use the find key to define a filter\n",
    "    \"COLUMNSMAP\":{'_id':'uuid',\n",
    "                  'source':'data_source',\n",
    "                  'name':'asset_name',\n",
    "                  'id':'alt_asset_id',\n",
    "                  'symbol':'asset_symbol',\n",
    "                  'currency' : 'currency',\n",
    "                  'price.date':'price_date',\n",
    "                  'price.value':'price_value',\n",
    "                  'mcap.date':'mcap_date',\n",
    "                  'mcap.value':'mcap_value',\n",
    "                  'volume.date':'volume_date',\n",
    "                  'volume.size':'volume_size',\n",
    "#                   'audit.mod.by':'created_by',\n",
    "#                   'audit.mod.date':'created_dt',\n",
    "                 }\n",
    "}\n",
    "_data = clsMC.nosql_to_sql(\n",
    "    source_db=__SOURCEDBNAME__,\n",
    "#     coll_list=[],\n",
    "    destin_db=__DESTINDBNAME__,\n",
    "    table_name=__DESTINTABLE__,\n",
    "    **_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2848c303",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101370"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3379*30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d6c601",
   "metadata": {},
   "source": [
    "## Spark MongoDB experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "33eda850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/16 15:15:19 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "root\n",
      " |-- _id: struct (nullable = true)\n",
      " |    |-- oid: string (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- marketcap: double (nullable = true)\n",
      " |-- source: string (nullable = true)\n",
      " |-- symbol: string (nullable = true)\n",
      "\n",
      "+--------------------+-------------------+--------------------+---------+------+\n",
      "|                 _id|               date|           marketcap|   source|symbol|\n",
      "+--------------------+-------------------+--------------------+---------+------+\n",
      "|{6390a3bf9d51d344...|2022-07-01 16:00:00|3.659842243234527...|coingecko|   btc|\n",
      "|{6390a3bf9d51d344...|2022-07-02 16:00:00|3.709165299187163E11|coingecko|   btc|\n",
      "|{6390a3bf9d51d344...|2022-07-03 16:00:00| 3.67706689137568E11|coingecko|   btc|\n",
      "|{6390a3bf9d51d344...|2022-07-04 16:00:00|3.685493641350522...|coingecko|   btc|\n",
      "|{6390a3bf9d51d344...|2022-07-05 16:00:00|3.864251762627784...|coingecko|   btc|\n",
      "|{6390a3bf9d51d344...|2022-07-06 16:00:00|3.853017830657297...|coingecko|   btc|\n",
      "|{6390a3bf9d51d344...|2022-07-07 16:00:00|3.924798330706018E11|coingecko|   btc|\n",
      "|{6390a3bf9d51d344...|2022-07-08 16:00:00|4.140556385738548E11|coingecko|   btc|\n",
      "|{6390a3bf9d51d344...|2022-07-09 16:00:00|4.176902865045072...|coingecko|   btc|\n",
      "|{6390a3bf9d51d344...|2022-07-10 16:00:00|4.124598387382113E11|coingecko|   btc|\n",
      "|{6390a3bf9d51d344...|2022-07-11 16:00:00|3.982495458886735E11|coingecko|   btc|\n",
      "|{6390a3bf9d51d344...|2022-07-12 16:00:00|3.820002903223988...|coingecko|   btc|\n",
      "|{6390a3bf9d51d344...|2022-07-13 16:00:00|3.701917672905106E11|coingecko|   btc|\n",
      "|{6390a3bf9d51d344...|2022-07-14 16:00:00|3.854625665489708E11|coingecko|   btc|\n",
      "|{6390a3bf9d51d344...|2022-07-15 16:00:00|3.931043467027113...|coingecko|   btc|\n",
      "|{6390a3bf9d51d344...|2022-07-16 16:00:00|3.972852330656553...|coingecko|   btc|\n",
      "|{6390a3bf9d51d344...|2022-07-17 16:00:00|4.047056425808907...|coingecko|   btc|\n",
      "|{6390a3bf9d51d344...|2022-07-18 16:00:00|3.976606105605498E11|coingecko|   btc|\n",
      "|{6390a3bf9d51d344...|2022-07-19 16:00:00|4.257567366606174E11|coingecko|   btc|\n",
      "|{6390a3bf9d51d344...|2022-07-20 16:00:00|4.458890132511064E11|coingecko|   btc|\n",
      "+--------------------+-------------------+--------------------+---------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "_appName = \" \".join(\n",
    "    [clsNoSQL.__app__,\n",
    "     clsNoSQL.__name__,\n",
    "     clsNoSQL.__package__,\n",
    "     clsNoSQL.__module__\n",
    "    ])   #\"PySpark MongoDB Examples\"\n",
    "_master = clsNoSQL.sparkMaster  # \"local[1]\"\n",
    "_h_ip = clsNoSQL.dbHostIP #\"127.0.0.1\"\n",
    "_type = clsNoSQL.dbType   # mongodb\n",
    "_port = clsNoSQL.dbPort   # 27017\n",
    "_format = clsNoSQL.dbFormat # mongo\n",
    "_user = clsNoSQL.dbUser   # \"farmraider\"\n",
    "_pswd = clsNoSQL.dbPswd   # \"spirittribe\"\n",
    "_auth = clsNoSQL.dbAuthSource   #\"tip-historic-marketcap\"\n",
    "_db = clsNoSQL.dbName     # \"tip-historic-marketcap\"\n",
    "_coll = 'coingecko.2022-07-01.btc'\n",
    "\n",
    "_inp_uri = f\"{_type}://{_user}:{_pswd}@{_h_ip}/{_db}.{_coll}?authSource={_auth}\"\n",
    "_out_uri = f\"{_type}://{_user}:{_pswd}@{_h_ip}/{_db}.{_coll}?authSource={_auth}\"\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(_appName) \\\n",
    "    .master(_master) \\\n",
    "    .config(\"spark.mongodb.input.uri\", _inp_uri) \\\n",
    "    .config(\"spark.mongodb.output.uri\", _out_uri) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read data from MongoDB\n",
    "df = spark.read.format(_format).load()\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fda3821",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
