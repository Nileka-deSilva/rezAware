{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "149b7738",
   "metadata": {},
   "source": [
    "# Derive Top N Portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "115dba92",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    WARNING CONTROL to display or ignore all warnings\n",
    "'''\n",
    "import warnings; warnings.simplefilter('ignore')     #switch betweeb 'default' and 'ignore'\n",
    "import traceback\n",
    "\n",
    "''' Set debug flag to view extended error messages; else set it to False to turn off debugging mode '''\n",
    "debug = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46d5864c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All functional SPARKDBWLS-libraries in LOAD-package of ETL-module imported successfully!\n",
      "All functional SPARKCLEANNRICH-libraries in TRANSFORM-package of ETL-module imported successfully!\n",
      "All functional DATAPREP-libraries in ETL-package of ASSETS-module imported successfully!\n",
      "sparkNoSQLwls Class initialization complete\n",
      "dataPrep Class initialization complete\n",
      "\n",
      "Class initialization and load complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from datetime import datetime, date, timedelta\n",
    "\n",
    "sys.path.insert(1,\"/home/nuwan/workspace/rezaware/\")\n",
    "# import rezaware as reza\n",
    "from utils.modules.etl.load import sparkDBwls as sdb\n",
    "from utils.modules.etl.transform import sparkCleanNRich as scne\n",
    "from wrangler.modules.assets.etl import dataPrep as prep\n",
    "# from utils.modules.ml.timeseries import rollingstats as stats\n",
    "\n",
    "''' restart initiate classes '''\n",
    "if debug:\n",
    "    import importlib\n",
    "#     reza = importlib.reload(reza)\n",
    "    sdb = importlib.reload(sdb)\n",
    "    scne = importlib.reload(scne)\n",
    "    prep = importlib.reload(prep)\n",
    "#     stats= importlib.reload(stats)\n",
    "    \n",
    "__desc__ = \"analyze crypto market capitalization time series data\"\n",
    "# clsSDB = sdb.SQLWorkLoads(desc=__desc__)\n",
    "clsSCNR=scne.Transformer(desc=__desc__)\n",
    "# clsStat=stats.RollingStats(desc=__desc__)\n",
    "clsPrep =prep.RateOfReturns(desc=__desc__)\n",
    "# ''' optional - if not specified class will use the default values '''\n",
    "# prop_kwargs = {\"WRITE_TO_TMP\":True,   # necessary to emulate the etl dag\n",
    "#               }\n",
    "print(\"\\nClass initialization and load complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0dd26e",
   "metadata": {},
   "source": [
    "## Read data from mcap_past\n",
    "We apply a query to select assets with mcap > 1.0 million. Any missing values are imputed with the mean value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b19fd615",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "_fpath = os.path.join('/home/nuwan/workspace/rezaware/',\n",
    "                      'wrangler/data/assets/etl/tmp/assets_20220720.csv')\n",
    "_piv_asset_list = pd.read_csv(_fpath)['asset_name'].tolist()\n",
    "# _piv_asset_list[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "76c04b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 15819:>                                                      (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5083 rows and 10 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "_from_date = '2022-07-20'\n",
    "_to_date = '2022-07-31'\n",
    "_value_limit=100000000\n",
    "_attr_prefix = 'mcap'\n",
    "_piv_cols = _piv_asset_list[3500:]\n",
    "\n",
    "_kwargs = {\n",
    "    \"DBNAME\" : \"tip\",\n",
    "    \"TABLENAME\" : 'mcap_past',\n",
    "    \"PARTCOLNAME\":'asset_name',\n",
    "    \"DATECOLNAME\":\"_\".join([_attr_prefix,'date']),\n",
    "    \"VALUECOLNAME\":\"_\".join([_attr_prefix,'value']),\n",
    "    \"AGGREGATE\":'avg',\n",
    "    \"IMPUTESTRATEGY\":'mean',\n",
    "    \"PIVCOLUMNS\":_piv_cols,\n",
    "}\n",
    "\n",
    "# _query = \"select * from warehouse.mcap_past wmp \"+\\\n",
    "#         f\"where wmp.mcap_date between '{_from_date}' and '{_to_date}' \"+\\\n",
    "#         f\"and wmp.mcap_value > 10000 \"\n",
    "_query =f\"SELECT wmp.mcap_past_pk, wmp.uuid, wmp.asset_symbol, \"+\\\n",
    "        f\"wmp.{_kwargs['DATECOLNAME']}, wmp.{_kwargs['VALUECOLNAME']}, \"+\\\n",
    "        f\"wmp.{_kwargs['PARTCOLNAME']}, wmp.currency, \"+\\\n",
    "        f\"wmp.created_dt,wmp.created_by,wmp.created_proc \"+\\\n",
    "        f\"FROM warehouse.mcap_past wmp WHERE 1=1 \"+\\\n",
    "        f\"AND wmp.{_kwargs['DATECOLNAME']} between '{_from_date}' AND '{_to_date}' \"+\\\n",
    "        f\"AND wmp.mcap_value > {_value_limit} \"+\\\n",
    "        f\"AND wmp.deactivate_dt IS NULL\"\n",
    "# print(_query)\n",
    "mcap_sdf = clsPrep.read_n_clean_mcap(query=_query,**_kwargs)\n",
    "\n",
    "print(\"Loaded %d rows and %d columns\" % (mcap_sdf.count(),len(mcap_sdf.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "89720a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows =  530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0----------------------------\n",
      " asset_name   | ake                  \n",
      " mcap_date    | 2022-07-20 00:00:00  \n",
      " mcap_past_pk | 511176               \n",
      " uuid         | 63945b519d51d3449... \n",
      " asset_symbol | cake                 \n",
      " currency     | null                 \n",
      " created_dt   | 2023-02-14 17:47:... \n",
      " created_by   | farmraider           \n",
      " created_proc | wrangler_assets_e... \n",
      " mcap_value   | 508889928.3761090... \n",
      "only showing top 1 row\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "mcap_sdf=mcap_sdf.filter(F.col('asset_name').isin(_piv_cols))\n",
    "print(\"rows = \",mcap_sdf.count())\n",
    "print(mcap_sdf.show(n=1,vertical=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b53a90a",
   "metadata": {},
   "source": [
    "## Compute LogROR for all assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0957f902",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------------------\n",
      " asset_name    | ala                  \n",
      " mcap_date     | 2022-07-20 00:00:00  \n",
      " mcap_past_pk  | 641030               \n",
      " uuid          | 6392d6fa9d51d3449... \n",
      " asset_symbol  | gala                 \n",
      " currency      | usd                  \n",
      " created_dt    | 2023-02-14 18:30:... \n",
      " created_by    | farmraider           \n",
      " created_proc  | wrangler_assets_e... \n",
      " mcap_value    | 445818989.2479350... \n",
      " mcap_simp_ror | null                 \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "_ror='SIMP'\n",
    "\n",
    "if _ror in ['NATLOG','LOG2','LOG10','LN']:\n",
    "    _ror_col=\"_\".join([_attr_prefix,'log','ror'])\n",
    "elif _ror=='SIMP':\n",
    "    _ror_col=\"_\".join([_attr_prefix,_ror.lower(),'ror'])\n",
    "else:\n",
    "    pass\n",
    "_kwargs[\"PREVALCOLNAME\"]=\"_\".join([_attr_prefix,'lag'])\n",
    "_kwargs[\"DIFFCOLNAME\"] = \"_\".join([_attr_prefix,'diff'])\n",
    "_kwargs[\"RORCOLNAME\"] = _ror_col\n",
    "_kwargs[\"PARTITIONS\"] = 2\n",
    "#     \"PARTCOLNAME\":'asset_name',\n",
    "#     \"DATECOLNAME\":\"_\".join([_attr_prefix,'date']),\n",
    "#     \"VALUECOLNAME\":\"_\".join([_attr_prefix,'value']),\n",
    "#     \"COLUMN\":\"_\".join([_attr_prefix,'date']),\n",
    "#     \"FROMDATETIME\":_from_date,\n",
    "#     \"TODATETIME\":_to_date,\n",
    "\n",
    "_ror_data, _ror_col = clsPrep.calc_ror(\n",
    "    data=mcap_sdf,\n",
    "    ror_type=_ror,\n",
    "    num_col =_kwargs['VALUECOLNAME'],\n",
    "    part_col=_kwargs['PARTCOLNAME'],\n",
    "    date_col=_kwargs['DATECOLNAME'],\n",
    "    **_kwargs,\n",
    ")\n",
    "\n",
    "_ror_data.filter(F.col(_kwargs[\"PARTCOLNAME\"]).isNotNull()).show(n=1,vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44431e4f",
   "metadata": {},
   "source": [
    "## Write ROR data to DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0d4c1343",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating upsert attributes and parameters ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wait a moment, writing data to postgresql tip database ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upserted 492 records\n"
     ]
    }
   ],
   "source": [
    "_upsert_sdf=_ror_data.select('*')\\\n",
    "    .filter(F.col(_kwargs[\"RORCOLNAME\"]).isNotNull())\n",
    "\n",
    "_records=clsPrep.write_data_to_db(\n",
    "    data=_upsert_sdf,\n",
    "#     **kwargs,\n",
    ")\n",
    "print(\"Upserted %d records\" % _records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df55024",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
