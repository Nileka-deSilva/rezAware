{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "149b7738",
   "metadata": {},
   "source": [
    "# Derive Top N Portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "115dba92",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    WARNING CONTROL to display or ignore all warnings\n",
    "'''\n",
    "import warnings; warnings.simplefilter('ignore')     #switch betweeb 'default' and 'ignore'\n",
    "import traceback\n",
    "\n",
    "''' Set debug flag to view extended error messages; else set it to False to turn off debugging mode '''\n",
    "debug = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46d5864c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All functional SPARKDBWLS-libraries in LOADER-package of ETL-module imported successfully!\n",
      "All functional SPARKCLEANNRICH-libraries in TRANSFORM-package of ETL-module imported successfully!\n",
      "All functional DATAPREP-libraries in ETL-package of ASSETS-module imported successfully!\n",
      "All functional SPARKDBWLS-libraries in LOADER-package of ETL-module imported successfully!\n",
      "All functional SPARKCLEANNRICH-libraries in TRANSFORM-package of ETL-module imported successfully!\n",
      "All functional DATAPREP-libraries in ETL-package of ASSETS-module imported successfully!\n",
      "All functional APP-libraries in REZAWARE-package of REZAWARE-module imported successfully!\n",
      "All functional SPARKNOSQLWLS-libraries in LOADER-package of ETL-module imported successfully!\n",
      "sparkNoSQLwls Class initialization complete\n",
      "dataPrep Class initialization complete\n",
      "\n",
      "Class initialization and load complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from datetime import datetime, date, timedelta\n",
    "\n",
    "sys.path.insert(1,\"/home/nuwan/workspace/rezaware/\")\n",
    "# import rezaware as reza\n",
    "from utils.modules.etl.loader import sparkDBwls as sdb\n",
    "from utils.modules.etl.transform import sparkCleanNRich as scne\n",
    "from wrangler.modules.assets.etl import dataPrep as prep\n",
    "# from utils.modules.ml.timeseries import rollingstats as stats\n",
    "\n",
    "''' restart initiate classes '''\n",
    "if debug:\n",
    "    import importlib\n",
    "#     reza = importlib.reload(reza)\n",
    "    sdb = importlib.reload(sdb)\n",
    "    scne = importlib.reload(scne)\n",
    "    prep = importlib.reload(prep)\n",
    "#     stats= importlib.reload(stats)\n",
    "    \n",
    "__desc__ = \"analyze crypto market capitalization time series data\"\n",
    "# clsSDB = sdb.SQLWorkLoads(desc=__desc__)\n",
    "clsSCNR=scne.Transformer(desc=__desc__)\n",
    "# clsStat=stats.RollingStats(desc=__desc__)\n",
    "clsPrep =prep.RateOfReturns(desc=__desc__)\n",
    "# ''' optional - if not specified class will use the default values '''\n",
    "# prop_kwargs = {\"WRITE_TO_TMP\":True,   # necessary to emulate the etl dag\n",
    "#               }\n",
    "print(\"\\nClass initialization and load complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b53a90a",
   "metadata": {},
   "source": [
    "## Batch process ROR\n",
    "Iterate through mcap and price data, for a given period, to compute the Log and Simple ROR.\n",
    "\n",
    "### Read data from mcap_past\n",
    "We apply a query to select assets with mcap > 1.0 million. Any missing values are imputed with the mean value.\n",
    "\n",
    "### Compute LogROR for all assets\n",
    "\n",
    "### Save ROR in DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8980e6e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/17 11:09:20 WARN DAGScheduler: Broadcasting large task binary with size 1721.8 KiB\n",
      "23/05/17 11:09:21 WARN DAGScheduler: Broadcasting large task binary with size 1732.1 KiB\n",
      "23/05/17 11:09:29 WARN DAGScheduler: Broadcasting large task binary with size 1721.8 KiB\n",
      "23/05/17 11:09:29 WARN DAGScheduler: Broadcasting large task binary with size 1732.1 KiB\n",
      "23/05/17 11:09:45 WARN DAGScheduler: Broadcasting large task binary with size 1722.7 KiB\n",
      "23/05/17 11:09:46 WARN DAGScheduler: Broadcasting large task binary with size 1733.0 KiB\n",
      "23/05/17 11:09:54 WARN DAGScheduler: Broadcasting large task binary with size 1722.7 KiB\n",
      "23/05/17 11:09:55 WARN DAGScheduler: Broadcasting large task binary with size 1733.0 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating upsert attributes and parameters ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/17 11:10:04 WARN DAGScheduler: Broadcasting large task binary with size 1722.7 KiB\n",
      "23/05/17 11:10:05 WARN DAGScheduler: Broadcasting large task binary with size 1733.0 KiB\n",
      "23/05/17 11:10:13 WARN DAGScheduler: Broadcasting large task binary with size 1722.7 KiB\n",
      "23/05/17 11:10:14 WARN DAGScheduler: Broadcasting large task binary with size 1733.0 KiB\n",
      "23/05/17 11:10:24 WARN DAGScheduler: Broadcasting large task binary with size 1722.7 KiB\n",
      "23/05/17 11:10:24 WARN DAGScheduler: Broadcasting large task binary with size 1733.0 KiB\n",
      "23/05/17 11:10:33 WARN DAGScheduler: Broadcasting large task binary with size 1722.7 KiB\n",
      "23/05/17 11:10:34 WARN DAGScheduler: Broadcasting large task binary with size 1733.0 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wait a moment, writing data to postgresql tip database ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/17 11:10:42 WARN DAGScheduler: Broadcasting large task binary with size 1725.4 KiB\n",
      "23/05/17 11:10:42 WARN DAGScheduler: Broadcasting large task binary with size 1741.6 KiB\n",
      "23/05/17 11:10:43 WARN DAGScheduler: Broadcasting large task binary with size 1749.5 KiB\n",
      "23/05/17 11:10:43 WARN DAGScheduler: Broadcasting large task binary with size 1792.0 KiB\n",
      "23/05/17 11:10:53 WARN DAGScheduler: Broadcasting large task binary with size 1722.7 KiB\n",
      "23/05/17 11:10:54 WARN DAGScheduler: Broadcasting large task binary with size 1733.0 KiB\n",
      "23/05/17 11:11:02 WARN DAGScheduler: Broadcasting large task binary with size 1722.7 KiB\n",
      "23/05/17 11:11:02 WARN DAGScheduler: Broadcasting large task binary with size 1733.0 KiB\n",
      "23/05/17 11:11:12 WARN DAGScheduler: Broadcasting large task binary with size 1722.7 KiB\n",
      "23/05/17 11:11:12 WARN DAGScheduler: Broadcasting large task binary with size 1733.0 KiB\n",
      "23/05/17 11:11:27 WARN DAGScheduler: Broadcasting large task binary with size 1723.9 KiB\n",
      "23/05/17 11:11:28 WARN DAGScheduler: Broadcasting large task binary with size 1734.7 KiB\n",
      "23/05/17 11:11:43 WARN DAGScheduler: Broadcasting large task binary with size 1723.9 KiB\n",
      "23/05/17 11:11:44 WARN DAGScheduler: Broadcasting large task binary with size 1734.7 KiB\n"
     ]
    }
   ],
   "source": [
    "__from_date__=date(2023,3,1)\n",
    "__to_date__ = date(2023,3,31)\n",
    "__val_lim__ = 10000000\n",
    "__bat_size__= 100\n",
    "\n",
    "kwargs={}\n",
    "kwargs={\n",
    "    \"RORTYPE\" : [\"LOG2\"],\n",
    "    \"VALCOLPREFIX\":[\"price\"],\n",
    "}\n",
    "\n",
    "ror_data = clsPrep.enrich_with_ror(\n",
    "    from_date=__from_date__,\n",
    "    to_date = __to_date__,\n",
    "    batch_size =__bat_size__,\n",
    "    value_limit=__val_lim__,\n",
    "    **kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b3f6330f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15701"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clsPrep.data.count()\n",
    "# clsPrep.data.show(n=3,vertical=True)\n",
    "# clsPrep.data.write.options(header='True', delimiter=',') \\\n",
    "#  .csv(\"/tmp/upsert_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44431e4f",
   "metadata": {},
   "source": [
    "## Write ROR data to DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0d4c1343",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating upsert attributes and parameters ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wait a moment, writing data to postgresql tip database ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upserted 492 records\n"
     ]
    }
   ],
   "source": [
    "''' DEPRECATED added batch process to dataPrep'''\n",
    "\n",
    "_upsert_sdf=_ror_data.select('*')\\\n",
    "    .filter(F.col(_kwargs[\"RORCOLNAME\"]).isNotNull())\n",
    "\n",
    "_records=clsPrep.write_data_to_db(\n",
    "    data=_upsert_sdf,\n",
    "#     **kwargs,\n",
    ")\n",
    "print(\"Upserted %d records\" % _records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d62dc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' DEPRECATED added batch process to dataPrep'''\n",
    "import pandas as pd\n",
    "_fpath = os.path.join('/home/nuwan/workspace/rezaware/',\n",
    "                      'wrangler/data/assets/etl/tmp/assets_20220720.csv')\n",
    "_piv_asset_list = pd.read_csv(_fpath)['asset_name'].tolist()\n",
    "# _piv_asset_list[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "89720a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows =  530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0----------------------------\n",
      " asset_name   | ake                  \n",
      " mcap_date    | 2022-07-20 00:00:00  \n",
      " mcap_past_pk | 511176               \n",
      " uuid         | 63945b519d51d3449... \n",
      " asset_symbol | cake                 \n",
      " currency     | null                 \n",
      " created_dt   | 2023-02-14 17:47:... \n",
      " created_by   | farmraider           \n",
      " created_proc | wrangler_assets_e... \n",
      " mcap_value   | 508889928.3761090... \n",
      "only showing top 1 row\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "''' DEPRECATED added batch process to dataPrep'''\n",
    "from pyspark.sql import functions as F\n",
    "mcap_sdf=mcap_sdf.filter(F.col('asset_name').isin(_piv_cols))\n",
    "print(\"rows = \",mcap_sdf.count())\n",
    "print(mcap_sdf.show(n=1,vertical=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76c04b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/06 13:19:33 WARN DAGScheduler: Broadcasting large task binary with size 1738.6 KiB\n",
      "23/05/06 13:19:41 WARN DAGScheduler: Broadcasting large task binary with size 1738.6 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 29794 rows and 10 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 2548:================================================>       (6 + 1) / 7]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "''' DEPRECATED added batch process to dataPrep'''\n",
    "\n",
    "_from_date = '2023-03-01'\n",
    "_to_date = '2023-03-31'\n",
    "_value_limit=10000000\n",
    "_attr_prefix = 'mcap'\n",
    "# _piv_cols = _piv_asset_list[3500:]\n",
    "\n",
    "_kwargs = {\n",
    "    \"DBNAME\" : \"tip\",\n",
    "    \"TABLENAME\" : 'mcap_past',\n",
    "    \"PARTCOLNAME\":'asset_name',\n",
    "    \"DATECOLNAME\":\"_\".join([_attr_prefix,'date']),\n",
    "    \"VALUECOLNAME\":\"_\".join([_attr_prefix,'value']),\n",
    "    \"AGGREGATE\":'avg',\n",
    "    \"IMPUTESTRATEGY\":'mean',\n",
    "    \"BATCHSIZE\":100,\n",
    "#     \"PIVCOLUMNS\":_piv_cols,\n",
    "}\n",
    "\n",
    "# _query = \"select * from warehouse.mcap_past wmp \"+\\\n",
    "#         f\"where wmp.mcap_date between '{_from_date}' and '{_to_date}' \"+\\\n",
    "#         f\"and wmp.mcap_value > 10000 \"\n",
    "_query =f\"SELECT wmp.mcap_past_pk, wmp.uuid, wmp.asset_symbol, \"+\\\n",
    "        f\"wmp.{_kwargs['DATECOLNAME']}, wmp.{_kwargs['VALUECOLNAME']}, \"+\\\n",
    "        f\"wmp.{_kwargs['PARTCOLNAME']}, wmp.currency, \"+\\\n",
    "        f\"wmp.created_dt,wmp.created_by,wmp.created_proc \"+\\\n",
    "        f\"FROM warehouse.mcap_past wmp WHERE 1=1 \"+\\\n",
    "        f\"AND wmp.{_kwargs['DATECOLNAME']} between '{_from_date}' AND '{_to_date}' \"+\\\n",
    "        f\"AND wmp.mcap_value > {_value_limit} \"+\\\n",
    "        f\"AND wmp.deactivate_dt IS NULL\"\n",
    "# print(_query)\n",
    "mcap_sdf = clsPrep.read_n_clean_mcap(query=_query,**_kwargs)\n",
    "\n",
    "print(\"Loaded %d rows and %d columns\" % (mcap_sdf.count(),len(mcap_sdf.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0957f902",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------------------\n",
      " asset_name    | ala                  \n",
      " mcap_date     | 2022-07-20 00:00:00  \n",
      " mcap_past_pk  | 641030               \n",
      " uuid          | 6392d6fa9d51d3449... \n",
      " asset_symbol  | gala                 \n",
      " currency      | usd                  \n",
      " created_dt    | 2023-02-14 18:30:... \n",
      " created_by    | farmraider           \n",
      " created_proc  | wrangler_assets_e... \n",
      " mcap_value    | 445818989.2479350... \n",
      " mcap_simp_ror | null                 \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "''' DEPRECATED added batch process to dataPrep'''\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "_ror='SIMP'\n",
    "\n",
    "if _ror in ['NATLOG','LOG2','LOG10','LN']:\n",
    "    _ror_col=\"_\".join([_attr_prefix,'log','ror'])\n",
    "elif _ror=='SIMP':\n",
    "    _ror_col=\"_\".join([_attr_prefix,_ror.lower(),'ror'])\n",
    "else:\n",
    "    pass\n",
    "_kwargs[\"PREVALCOLNAME\"]=\"_\".join([_attr_prefix,'lag'])\n",
    "_kwargs[\"DIFFCOLNAME\"] = \"_\".join([_attr_prefix,'diff'])\n",
    "_kwargs[\"RORCOLNAME\"] = _ror_col\n",
    "_kwargs[\"PARTITIONS\"] = 2\n",
    "#     \"PARTCOLNAME\":'asset_name',\n",
    "#     \"DATECOLNAME\":\"_\".join([_attr_prefix,'date']),\n",
    "#     \"VALUECOLNAME\":\"_\".join([_attr_prefix,'value']),\n",
    "#     \"COLUMN\":\"_\".join([_attr_prefix,'date']),\n",
    "#     \"FROMDATETIME\":_from_date,\n",
    "#     \"TODATETIME\":_to_date,\n",
    "\n",
    "_ror_data, _ror_col = clsPrep.calc_ror(\n",
    "    data=mcap_sdf,\n",
    "    ror_type=_ror,\n",
    "    num_col =_kwargs['VALUECOLNAME'],\n",
    "    part_col=_kwargs['PARTCOLNAME'],\n",
    "    date_col=_kwargs['DATECOLNAME'],\n",
    "    **_kwargs,\n",
    ")\n",
    "\n",
    "_ror_data.filter(F.col(_kwargs[\"PARTCOLNAME\"]).isNotNull()).show(n=1,vertical=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
